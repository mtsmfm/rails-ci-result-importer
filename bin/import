#! /usr/bin/env ruby
require "bundler/inline"
require "open3"
require "json"
require "logger"

gemfile do
  source 'https://rubygems.org'

  gem "sentry-raven"
  gem "redis"
  gem "unofficial_buildkite_client"
  gem "google-cloud-bigquery", require: "google/cloud/bigquery"
end

class Workflow
  FIRST_BUILD_NUMBER = 58431
  BATCH_SIZE = (ENV['BATCH_SIZE'] || 1).to_i
  REDIS_FAILED_DATA_KEY = 'failed_data'

  def run
    latest_imported_build_number = bigquery_dataset.query("SELECT MAX(build_number) as num FROM `rails_ci_result.buildkite_jobs`").dig(0, :num) || 1
    latest_finished_build_number = buildkite_client.fetch_builds(first: 1, state: %w(PASSED FAILED)).first[:number]

    failed_data = JSON.parse(redis.get(REDIS_FAILED_DATA_KEY) || '{}')
    ignored_build_numbers = failed_data.select {|_, count| count >= 2 }.keys

    not_imported_build_numbers = bigquery_dataset.query(<<~SQL).map {|build_number:| build_number }
      WITh X AS (SELECT build_number FROM UNNEST(GENERATE_ARRAY(#{FIRST_BUILD_NUMBER}, (SELECT MAX(build_number) FROM `rails_ci_result.buildkite_jobs`))) AS build_number)
      SELECT build_number FROM X #{ignored_build_numbers.empty? ? "" : "WHERE build_number NOT IN (#{ignored_build_numbers.join(",")})"}
      EXCEPT DISTINCT SELECT build_number FROM `rails_ci_result.buildkite_jobs` ORDER BY build_number DESC LIMIT #{BATCH_SIZE};
    SQL

    ((latest_imported_build_number..latest_finished_build_number).to_a + not_imported_build_numbers).uniq.sort.last(BATCH_SIZE).each do |build_number|
      logger.info("Start build_number:[#{build_number}]")
      Raven.tags_context(build_number: build_number)

      Bundler.with_original_env do
        output, status = Open3.capture2e("BUILDKITE_IMPORT_TARGET_BUILD_NUMS=#{build_number} embulk -J-Xmx500m run config.yml.liquid --bundle .")
        if status.success?
          failed_data.delete(build_number.to_s)
        else
          logger.error(output)
          Raven.capture_message(output.byteslice(-Raven::Event::MAX_MESSAGE_SIZE_IN_BYTES..-1), backtrace: output.lines.last(100), fingerprint: %w(ImportError error))

          failed_data[build_number.to_s] ||= 0
          failed_data[build_number.to_s] += 1
        end
      end
    end

    redis.set(REDIS_FAILED_DATA_KEY, failed_data.to_json)
  end

  private

  def buildkite_client
    @buildkite_client ||= UnofficialBuildkiteClient.new(org_slug: "rails", pipeline_slug: "rails")
  end

  def bigquery_client
    @bigquery_client ||= Google::Cloud::Bigquery.new
  end

  def bigquery_dataset
    @bigquery_dataset ||= bigquery_client.dataset("rails_ci_result")
  end

  def redis
    @redis ||= Redis.new
  end

  def logger
    @logger ||= Logger.new(STDOUT)
  end
end

Workflow.new.run
